#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†å™¨
åŠŸèƒ½ï¼šå®Œæ•´çš„è‡ªåŠ¨åŒ–æµç¨‹ - ä»å…³é”®è¯åˆ°å‘å¸ƒ
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from keyword_processor import KeywordProcessor
from article_generator import ArticleGenerator
from content_publisher import ContentPublisher
import json
import time

load_dotenv()

class BatchProcessor:
    def __init__(self):
        self.keyword_processor = KeywordProcessor()
        self.article_generator = ArticleGenerator()
        self.content_publisher = ContentPublisher()
        
        # é…ç½®
        self.batch_size = 10  # æ¯æ‰¹å¤„ç†æ•°é‡
        self.daily_limit = int(os.getenv('DEFAULT_ARTICLES_PER_DAY', 2))
    
    def process(self, keywords_file: str, output_dir: str = None, 
               max_articles: int = None, publish: bool = False):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        output_dir = Path(output_dir) if output_dir else Path('../data')
        keywords_processed_dir = output_dir / 'keywords-processed'
        articles_dir = output_dir / 'articles-generated'
        
        keywords_processed_dir.mkdir(parents=True, exist_ok=True)
        articles_dir.mkdir(parents=True, exist_ok=True)
        
        print("=" * 60)
        print("ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æµç¨‹")
        print("=" * 60)
        
        # æ­¥éª¤1: å¤„ç†å…³é”®è¯
        print("\nğŸ“Š æ­¥éª¤1: å¤„ç†å…³é”®è¯")
        print("-" * 60)
        keywords_processed_file = keywords_processed_dir / 'keywords-processed.json'
        df_keywords = self.keyword_processor.process(
            keywords_file, 
            str(keywords_processed_file)
        )
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        keywords_list = df_keywords.to_dict('records')
        
        # é™åˆ¶æ•°é‡
        if max_articles:
            keywords_list = keywords_list[:max_articles]
        
        print(f"\nğŸ“ æ­¥éª¤2: ç”Ÿæˆæ–‡ç«  (å…± {len(keywords_list)} ç¯‡)")
        print("-" * 60)
        
        # æ­¥éª¤2: ç”Ÿæˆæ–‡ç« ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
        generated_count = 0
        failed_count = 0
        
        for i in range(0, len(keywords_list), self.batch_size):
            batch = keywords_list[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(keywords_list) + self.batch_size - 1) // self.batch_size
            
            print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches} (å…± {len(batch)} ä¸ªå…³é”®è¯)")
            
            for j, keyword_data in enumerate(batch, 1):
                keyword = keyword_data['keyword']
                print(f"\n[{j}/{len(batch)}] {keyword}")
                
                try:
                    # ç”Ÿæˆæ–‡ç« 
                    article = self.article_generator.generate(keyword_data)
                    
                    # ä¿å­˜æ–‡ç« 
                    article_file = articles_dir / f"{article['slug']}.json"
                    with open(article_file, 'w', encoding='utf-8') as f:
                        json.dump(article, f, ensure_ascii=False, indent=2)
                    
                    generated_count += 1
                    print(f"   âœ… ç”ŸæˆæˆåŠŸ: {article_file.name}")
                    
                    # é¿å…APIé™æµ
                    time.sleep(2)
                    
                except Exception as e:
                    failed_count += 1
                    print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
                    continue
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if i + self.batch_size < len(keywords_list):
                print(f"\nâ¸ï¸  æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯10ç§’...")
                time.sleep(10)
        
        print(f"\nâœ… æ–‡ç« ç”Ÿæˆå®Œæˆï¼")
        print(f"   æˆåŠŸ: {generated_count} ç¯‡")
        print(f"   å¤±è´¥: {failed_count} ç¯‡")
        
        # æ­¥éª¤3: å‘å¸ƒæ–‡ç« 
        if publish and generated_count > 0:
            print(f"\nğŸ“¤ æ­¥éª¤3: å‘å¸ƒæ–‡ç« ")
            print("-" * 60)
            self.content_publisher.batch_publish(
                str(articles_dir),
                publish_now=False,  # å®šæ—¶å‘å¸ƒ
                daily_limit=self.daily_limit
            )
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   å¤„ç†å…³é”®è¯: {len(keywords_list)} ä¸ª")
        print(f"   ç”Ÿæˆæ–‡ç« : {generated_count} ç¯‡")
        print(f"   å‘å¸ƒæ–‡ç« : {'æ˜¯' if publish else 'å¦'}")
        
        if publish:
            print(f"\nğŸ“… å‘å¸ƒè®¡åˆ’:")
            print(f"   æ¯å¤©å‘å¸ƒ: {self.daily_limit} ç¯‡")
            total_days = (generated_count + self.daily_limit - 1) // self.daily_limit
            print(f"   é¢„è®¡å®Œæˆ: {total_days} å¤©")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python batch-processor.py <å…³é”®è¯æ–‡ä»¶> [é€‰é¡¹]")
        print("é€‰é¡¹:")
        print("  --max <æ•°é‡>     æœ€å¤§ç”Ÿæˆæ–‡ç« æ•°")
        print("  --publish        è‡ªåŠ¨å‘å¸ƒ")
        print("  --output <ç›®å½•>  è¾“å‡ºç›®å½•")
        print("\nç¤ºä¾‹:")
        print("  python batch-processor.py ../data/keywords-raw.csv --max 50 --publish")
        sys.exit(1)
    
    processor = BatchProcessor()
    keywords_file = sys.argv[1]
    
    # è§£æå‚æ•°
    max_articles = None
    publish = False
    output_dir = None
    
    i = 2
    while i < len(sys.argv):
        if sys.argv[i] == '--max' and i + 1 < len(sys.argv):
            max_articles = int(sys.argv[i + 1])
            i += 2
        elif sys.argv[i] == '--publish':
            publish = True
            i += 1
        elif sys.argv[i] == '--output' and i + 1 < len(sys.argv):
            output_dir = sys.argv[i + 1]
            i += 2
        else:
            i += 1
    
    processor.process(keywords_file, output_dir, max_articles, publish)

